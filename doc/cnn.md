# [QuNet](README.md) - CNN

## Введение

В библиотеке  [QuNet](README.md) различные линейные архитетектуры с конволюционными 2d слоями создаются при помощи класса `CNN`.
Основным параметром его конструктора является строка `blocks`. 
Она состоит из разделённых пробелами "токенов" определяющих последовательность элементарных или составных слоёв сети.
Например, сеть ResNet18 создаётся следующим образом:
```python
cnn = CNN(input=3, blocks="(c64_7_2 n f  m3_2) 2r r128_3_2 r r256_3_2 r r512_3_2 r")
```
В необязательных круглых скобках находится последовательность элементарных слоёв:
* `c64_7_2` - это конволюция `Conv2d` в 64 канала с ядром 7 и шагом 2. 
* `n` - нормализационный слой, равный по умолчанию  `BatchNorm2d`.
* `f` - активационная функция  (по умолчанию это `relu`).
* `m3_2` - это `MaxPool2d` с ядром 3 и шагом 2.

Затем идут составные блоки.

Токен `2r` означает, что идёт последовательность двух одинаковых residual блоков `r`.
Каждый из них содержит по два конволюционных слоя с нормировкой и нелинейностю между ними. Они окружены петлёй skip-connection. Такие блоки не меняют размерности тензора.

Токен `r128_3_2` также является residual блоком. Первый его конволюционный слой увеличивает число каналов до 128 и благодаря шагу (stride=2) уменьшает в 2 раза высоту H и ширину W входного тензора `(B,C,H,W)`. Второй конволюционный слой блока также имеет 128 выходных каналов, ядро 3 и уже единичный страйд. 
Так как размерность тензора меняется: `(B,64,H,W) -> (B,128,H/2,W/2)`, то петля skip-connection содержит  конволюционный слой `Conv2d(64,128,1)`, который выравнивает размерности входного и входного тензоров.

Аналогичный смысл имеют остальные токены строки.  Параметр `input` задаёт число входных каналов изображения.

Полная модель, решающая задачу классификации на основе архитектуры resnet18, должна дополнительно содержать классификатор (токен `cnf` это объединение последовательности `c n f`):
```python
m = nn.Sequential(
    CNN(input=3, avg=True, flat=True,
        blocks="(cnf64_7_2  m3_2) 2r r128_3_2 r r256_3_2 r r512_3_2 r"),    
    MLP(input=512, output=10) )
```
Параметр `avg=True` означает добавление после blocks слоя  `AdaptiveAvgPool2d(1)`,
а параметр `flat=True` добавление `Flatten()`. На самом деле их указывать не обязательно, т.к. их значение по умолчанию равно `True`.

<hr>

## Элементарные слои

Главный элементарный токен, это токен конволюционного слоя `c[channels]_[kernel]_[stride]_[padding]`.
По умолчанию число выходных каналов `channels` равно числу входных каналов, ядро `kernel=3` и шаг `stride=1`.
Если `padding` не задан (так обычно будет для resnet-овских архитектур), то он принимается равным `padding = (kernel - 1) // 2`. Таким образом:
```
c64      - Conv2d(..., 64, kernel_size=3, padding=1)
c64_5    - Conv2d(..., 64, kernel_size=5, padding=2)
c64_7_2  - Conv2d(..., 64, kernel_size=7, padding=3, stride=2)
```
Обратим внимание, что если `kernel=3`, а `stride=2`, то для однозначности нужно указать оба параметра: `c64_3_2`, хотя по умолчанию `kernel=3`.

Токен нормировки слоя это просто символ `n` или `n1`, `n2`, `n3`, где число означает тип нормировочного слоя (см. ниже раздел "Нормализация").

Токен  `f` - функция активации (по умолчнию это `relu`, см раздел ниже "Активационная функция")

Ещё два токена `m` и `a` создают слои `MaxPool2d` и `AvgPool2d` соответственно. 
Они имеют параметры `m[kernel]_[stride]_[padding]`, со значениями по умолчанию:
`kernel=2`, `stride=2`, `padding=0`. 
Поэтому одиночный символ `m` уменьшит размеры изображения в два раза.

Пару пробелов можно сэкономить при помощи токена `cnf`, который означает последовательность трёх слоёв `"c n f"`, где, идущие после него параметры относятся к конволюционному слою.

Токен `d` - означает слой Dropout. По умолчанию это `Dropout2d` (`cfg.drop_d=2`). 
В конкретном месте можно изменить размерность дропаута, поставив `d1` для `Dropout`  и `d2` для `Dropout2d`.
Вероятность дропаута при из создании всегда равна 0.0 (т.е. слои фактически не работают)
Чтобы их активировать, необходимо подключить класс `Change` и, после создании модели, вызвать его статический метод `dropout`.
Ему первым параметром передаётся модель (или часть её), а вторым  - значение вероятности `p`.
Если это число, то для всех слоёв `DropoutXd` будет задано это значение. 
Можно также задать масив вещественных чисел по числу слоёв `DropoutXd`.
Если этот массив коороче, чем число слоёв, будет повторяться последнее значение в массиве


Пример:
```python
from qunet import CNN, Change, ModelState

m = CNN(input=1, blocks="cnf32 m d cnf64 m d")  # тоже что "c32 n f m c64 n f m"
Change.dropout(m, [0.1, 0.2])

s = ModelState(m)
s.layers(2, input_size=(1,1,28,28) )
```
Класс `ModelState` выдаст следующую структуру сети (естественно всегда рекомендуется её просматривать):
```
CNN:  (1, 1, 28, 28) -> (1, 64)
CNN                                            params            input -> output shapes            
├─ ModuleList                                                                    ->                
│  └─ Sequential                                                 (1, 1, 28, 28)  -> (1, 32, 28, 28)
│     └─ Conv2d(1->32, k:3, s:1, p:1, F)          288  ~   2% |  (1, 1, 28, 28)  -> (1, 32, 28, 28)
│     └─ BatchNorm2d(32)                           64         |  (1, 32, 28, 28) -> (1, 32, 28, 28)
│     └─ ReLU                                                    (1, 32, 28, 28) -> (1, 32, 28, 28)
│  └─ MaxPool2d(k:2, s:2, p:0)                                   (1, 32, 28, 28) -> (1, 32, 14, 14)
│  └─ Dropout2d(0.1)                                             (1, 32, 14, 14) -> (1, 32, 14, 14)
│  └─ Sequential                                                 (1, 32, 14, 14) -> (1, 64, 14, 14)
│     └─ Conv2d(32->64, k:3, s:1, p:1, F)      18,432  ~  97% |  (1, 32, 14, 14) -> (1, 64, 14, 14)
│     └─ BatchNorm2d(64)                          128  ~   1% |  (1, 64, 14, 14) -> (1, 64, 14, 14)
│     └─ ReLU                                                    (1, 64, 14, 14) -> (1, 64, 14, 14)
│  └─ MaxPool2d(k:2, s:2, p:0)                                   (1, 64, 14, 14) -> (1, 64, 7, 7)  
│  └─ Dropout2d(0.2)                                             (1, 64, 7, 7)   -> (1, 64, 7, 7)  
│  └─ AdaptiveAvgPool2d(1)                                       (1, 64, 7, 7)   -> (1, 64, 1, 1)  
│  └─ Flatten                                                    (1, 64, 1, 1)   -> (1, 64)        
```

Если `ModelState` вылетает при выводе слоёв layers, стоит его запустить без парметра `input_size`.
Например, возможно указано неверное число входных каналов `input` или размер входного изображения меньше, 
чем число понижающих блоков `m`. Ну и наконец, никто не отменял старого доброго `print(m)`.
<hr>

## Residual Block

Токен `r` или `r[channels]_[kernel]_[stride]` создаёт блок с петлёй обхода (skip-connection).
В общем случчае этот блок выглядит следующим образом:

<center>
<img src="img/resblock.png" style="width:800px"> 
</center>

Центральным преобразователем входного тензора является `block(x)`. 
Он состоит из двух конволюционных слоёв с нормировкой и нелинейностью между ними.
Если stride отличен от единицы, то возможны два варианта. 
По умолчанию (`cfg.stride_first=True`) в первом слое стоит `stride > 1`, а во втором `stride=1`.
Если же `cfg.stride_first=False`,  то наоборот:

```python
Conv2d(...,     channels, kern, stride)    |    Conv2d(...,     channels, kern,  1)
Norm(channels)                             |    Norm(channels)
ReLU()                                     |    ReLU()
Conv2d(channels, channels, kern, 1)        |    Conv2d(channels, channels, kern, stride)
```

В обоих случаях размер изображения уменьшается в 2 раза. Вариант по умолчанию принят resnetXX архитектурах.
При одинаковом числе параметров `cfg.stride_first=True`  работает быстрее (сразу изображение уменьшает), но может оказаться менее эффективным (второе преобразование над меньшим тензором).
Тип нормировки, определяется параметром `cfg.norm_inside=1`, а функция активации параметром `cfg.fun_inside="relu"`.

В простейшем случае (`r` без параметров) размерности тензора на входе и на выходе совпадают (не меняется, ни число каналов, ни размер изображения). Такой блок окружается простой петлёй `align(x) = x`.

Если размерности тензора на входе и на выхде не совпадают (`channels` отлично от числа входных каналов  или `stride > 1`),
то петля `align(x)` является конволюционным слоём, который выравнивает размерности:
```
Con2d(...,  channels, kern, stride)
Norm(channels)
```

Свойства множителя `gamma` определяется конфигом. По умолчанию  (`cfg.res=1`) это единичная константа.
При `cfg.res=2` - обучаемый скалярный параметр и `cfg.res=3`  обучаемый вектор  размерности `С` (по числу каналов).

После вычисления `align(x) + gamma * block( norm(x) )` может стоять нормировка, функция активации и дроаут.
Это регулируется параметрами конфига:
```python
stride_first  = True,   # делать stride>1 на первом Conv2d, иначе на последнем

drop_d = 2,             # тип Dropout для всех токенов `d` строки blocks
drop_d_inside = 0,      # тип Dropout между Conv2d в блоках ('r', 'b')
drop_d_after  = 2,      # тип Dropout после блоков ('r')

norm   = 1,             # тип нормировки для токенов 'n'
norm_inside   = 1,      # тип нормировки внутри блоков между Conv2d
norm_after    = 0,      # добавлять нормировку после блоков ('r')
            
fun           = 'relu', # активационная функция для токена `f`
fun_inside    = 'relu', # активационная функция для блоков ('r')
fun_after     = "",     # добавлять активационную функцию после блоков ('r')
            
res           = 1,      # тип gamma (1-константа, 2-обучаемое число, 3-обучаемый вектор)
gamma         = 0.,     # начальное значение gamma (для res > 1)
```

Сам по себе блок `block(x)` (без петли) можно добавить при помощи токена `b`.

<hr>


## Нормализация

Нормализационный слой на входе получает тензор `x = (B,C,H,W)` и возвращает его 
в нормализованном  виде при помощи его среднего и стандарного отклонения от среднего:
```python
x = ( (x-x.mean(dims))/x.std(dims) ) * alpha + beta
```
где `alpha`, `beta` обучаемые векторы размерности `C`, т.е. слой имеет `2*C` параметров требующих градиента (в начале обучения `alpha=1`, `beta=0`).
Во всех случаях `alpha, beta` имеют форму `(1,C,1,1)`, а при вычислении средних стоит флаг `keepdims=True`.
Размерности `dims` по которым происходит усреднение, зависят от типа номализационного слоя (параметр `norm` в cfg ResCNN):

* `norm = 1` - nn.**BatchNorm2d**(C) - среднее по батчу (`dims=B`). Такая нормировка  вычисляет средние значения яркости каждого пикселя по всем примерам батча в каждом канале.  Дополнительно этот слой сохраняет скользящие средние mean и std, которые затем используются в режиме `eval` даже если пример в батче один.
* `norm = 2` - **LayerNormChannels**(C) - среднее по каналам (`dims=C`). Эта нормировка рассматривет индекс `C` как индекс вектора признаков (каким он и является в конечном итоге). Среднее значение этого вектора делается нулевым, а std - единичным. Затем обычаемые константы его именяют различным образом для каждой компоненты вектора.
* `norm = 3` - nn.**InstanceNorm2d**(C) - среднее по изображению (`dims=H,W`). В этом случае для каждого примера и канала усредняются яроксти всех пикселей. Нормированные изображения подправляются константтами `alpha` и `beta` имеющих уникальные значения для каждого канала (но общие для всех пикселей и примеров)

Для всех блоков используется один и тот же активационный слой, определяемый параметром `cfg.norm=1`.
Он же будет поставлен на место буквы `n` при описании архитектуры.
После буквы `n` можно поставить номер нормализатора. В этом случае он изменится от значения по умолчанию.
```python
"c32 n"  # после конволюции будет нормализационный слой cfg.norm
"c32 n2" # после конволюции будет LayerNormChannels, независимо от значения cfg.norm
```

<hr>

## Активационная функция

Этот параметр является строкой, которая может принимать следующие значения:
```python
'relu', 'gelu', 'relu6', 'sigmoid', 'tanh', 'swish', 'hswish', 'hsigmoid'
```
Как и в случае нормализции функция задается единым параметром `cfg.fun="relu"` для всех блоков.
Однако, при желании, её можно указать явным образом со значением отличным от значения по умолчанию:
```python
"r n f"     # на место f будет поставлена активационная функция из cfg.fun
"r n gelu"  # независимо от cfg.fun тут будет использоваться функция GELU
```

<hr>

## Варианты CNN-архитектуры

При построении CNN-архитектуры необходимо учитывать размер входного тензора и желаемое число признаков на выходе сети (число каналов в последем слое `AdaptiveAvgPool2d`).
Кроме этого ширина и высота финального тезора (перед `AdaptiveAvgPool2d`) не должна быть слишком большой или слишком маленькой (1x1). В первом случае признаки будут по-прежнему фокусироваться на мелких деталях. Во втором вычисления последних конволюционных блоков может быть не эффективно. Например, если на вход конволюции с ядром 3 и паддингом 1 поступает тензор `(512,2,2)`, то в каждом "пикселе" при вычислении будет участвовать все 4 значимых пикселя и 5 нулей паддинга. Возможно в этом случае имеет смысл раньше вызвать `AdaptiveAvgPool2d` и добавить скрытый слой в классификаторе.

Хорошие готовые архитектуры иногда необходимо адаптировать под конкретную задачу. 
Напимер к CIFAR10 с изображениями `(3,32,32)` не подходит resnet18, у которой в самом первом слое происходит уменьшение изображения в 4: `(cnf64_7_2 m3_2)` - сначала конволюция с ядром 7 и страйдом 2, затем MaxPooling с ядром 3. Такая архитектура приведёт к ошибке (размер изображения станет меньше чем 1x1). Для этой задачи целесообразно изменить начало сети, например, на `(cnf64 m)`.



