# [QuNet](README.md) - CNN

## Введение

В библиотеке  [QuNet](README.md) различные линейные архитетектуры с конволюционными 2d слоями создаются при помощи класса `CNN`.
Основным параметром его конструктора является строка `blocks`. 
Она состоит из разделённых пробелами "токенов" определяющих последовательность элементарных или составных слоёв сети.
Например, сеть ResNet18 создаётся следующим образом:
```python
cnn = CNN(input=3, blocks="(c64_7_2 n f  m3_2) 2r r128_3_2 r r256_3_2 r r512_3_2 r")
```
В необязательных круглых скобках находится последовательность элементарных слоёв:
* `c64_7_2` - это конволюция `Conv2d` в 64 канала с ядром 7 и шагом 2. 
* `n` - нормализационный слой, равный по умолчанию  `BatchNorm2d`.
* `f` - активационная функция  (по умолчанию это `relu`).
* `m3_2` - это `MaxPool2d` с ядром 3 и шагом 2.

Затем идут составные блоки.

Токен `2r` означает, что идёт последовательность двух одинаковых residual блоков `r`.
Каждый из них содержит по два конволюционных слоя с нормировкой и нелинейностю между ними. Они окружены петлёй skip-connection. Такие блоки не меняют размерности тензора.

Токен `r128_3_2` также является residual блоком. Первый его конволюционный слой увеличивает число каналов до 128 и благодаря шагу (stride=2) уменьшает в 2 раза высоту H и ширину W входного тензора `(B,C,H,W)`. Второй конволюционный слой блока также имеет 128 выходных каналов, ядро 3 и уже единичный страйд. 
Так как размерность тензора меняется: `(B,64,H,W) -> (B,128,H/2,W/2)`, то петля skip-connection содержит  конволюционный слой `Conv2d(64,128,1)`, который выравнивает размерности входного и входного тензоров.

Аналогичный смысл имеют остальные токены строки.  Параметр `input` задаёт число входных каналов изображения.

Полная модель, решающая задачу классификации на основе архитектуры resnet18, должна дополнительно содержать классификатор:
```python
m = nn.Sequential(
    CNN(input=3, avg=True, flat=True,
        blocks="(c64_7_2 n f  m3_2) 2r r128_3_2 r r256_3_2 r r512_3_2 r"),    
    MLP(input=512, output=10) )
```
Параметр `avg=True` означает добавление после blocks слоя  `AdaptiveAvgPool2d(1)`,
а параметр `flat=True` добавление `Flatten()`. На самом деле их указывать не обязательно, т.к. их значение по умолчанию равно `True`.

<hr>

## Элементарные блоки

Главный элементарный токен, это токен конволюционного слоя `c[channels]_[kernel]_[stride]_[padding]`.
По умолчанию число выходных каналов `channels` равно числу входных каналов, ядро `kernel=3` и шаг `stride=1`.
Если `padding` не задан (так обычно будет для resnet-овских архитектур), то он принимается равным `padding = (kernel - 1) // 2`. Таким образом:
```
c64      - Conv2d(..., 64, kernel_size=3, padding=1)
c64_5    - Conv2d(..., 64, kernel_size=5, padding=2)
c64_7_2  - Conv2d(..., 64, kernel_size=7, padding=3, stride=2)
```
Обратим внимание, что если `kernel=3`, а `stride=2`, то для однозначности нужно указать оба параметра: `c64_3_2`.

Токен нормировки слоя это просто символ `n` или `n1`, `n2`, `n3`, где число означает тип нормировочного слоя (см. ниже раздел "Нормализация").

Токен  `f` - функция активации (по умолчнию это `relu`, см раздел ниже "Активационная функция")

Ещё два токена `m` и `a` создают слои `MaxPool2d` и `AvgPool2d` соответственно. 
Они имеют параметры `m[kernel]_[stride]_[padding]`, со значениями по умолчанию:
`kernel=2`, `stride=2`, `padding=0`. 
Поэтому одиночный символ `m` уменьшит размеры изображения в два раза.

Пару пробелов можно сэкономить при помощи токена `cnf`, который означает последовательность трёх слоёв 'c n f', где, идущие после него параметры? относятся к конволюционному слою.

Пример:
```python
m = CNN(input=1, blocks="cnf32 m cnf64 m")  # тоже что "c32 n f m c64 n f m"

s = ModelState(m)
s.layers(2, input_size=(1,1,28,28) )
```
Класс `ModelState` выдаст следующую структуру сети (естественно всегда рекомендуется её просматривать):
```
CNN:  (1, 1, 28, 28) -> (1, 64)
CNN                                                           params           data
├─ ModuleList                                                                                   ->
│  └─ Sequential                                                                (1, 1, 28, 28)  -> (1, 32, 26, 26)
│     └─ Conv2d(1->32, k:(3, 3), s:(1, 1), p:(0, 0), F)          288  ~   2% |  (1, 1, 28, 28)  -> (1, 32, 26, 26)
│     └─ BatchNorm2d(32)                                          64         |  (1, 32, 26, 26) -> (1, 32, 26, 26)
│     └─ ReLU                                                                   (1, 32, 26, 26) -> (1, 32, 26, 26)
│  └─ MaxPool2d(k:2, s:2, p:0)                                                  (1, 32, 26, 26) -> (1, 32, 13, 13)
│  └─ Sequential                                                                (1, 32, 13, 13) -> (1, 64, 11, 11)
│     └─ Conv2d(32->64, k:(3, 3), s:(1, 1), p:(0, 0), F)      18,432  ~  97% |  (1, 32, 13, 13) -> (1, 64, 11, 11)
│     └─ BatchNorm2d(64)                                         128  ~   1% |  (1, 64, 11, 11) -> (1, 64, 11, 11)
│     └─ ReLU                                                                   (1, 64, 11, 11) -> (1, 64, 11, 11)
│  └─ MaxPool2d(k:2, s:2, p:0)                                                  (1, 64, 11, 11) -> (1, 64, 5, 5)
│  └─ AdaptiveAvgPool2d                                                         (1, 64, 5, 5)   -> (1, 64, 1, 1)
│  └─ Flatten                                                                   (1, 64, 1, 1)   -> (1, 64)
====================================================================
trainable:                                                    18,912
```

Если `ModelState` вылетает при выводе слоёв layers, стоит его запустить без парметра `input_size`.
Например, возможно указано неверное число входных каналов `input` или размер входного изображения меньше, 
чем число понижающих блоков `m`. Ну и наконец, никто не отменял старого доброго `print(m)`.
<hr>

## Residual Block

<hr>


## Нормализация

Нормализационный слой на входе получает тензор `x = (B,C,H,W)` и возвращает его 
в нормализованном  виде при помощи его среднего и стандарного отклонения от среднего:
```python
x = ( (x-x.mean(dims))/x.std(dims) ) * alpha + beta
```
где `alpha`, `beta` обучаемые векторы размерности `C`, т.е. слой имеет `2*C` параметров требующих градиента (в начале обучения `alpha=1`, `beta=0`).
Во всех случаях `alpha, beta` имеют форму `(1,C,1,1)`, а при вычислении средних стоит флаг `keepdims=True`.
Размерности `dims` по которым происходит усреднение, зависят от типа номализационного слоя (параметр `norm` в cfg ResCNN):

* `norm == 1` - nn.**BatchNorm2d**(C) - среднее по батчу (`dims=B`). Такая нормировка  вычисляет средние значения яркости каждого пикселя по всем примерам батча в каждом канале. Это анлогично нормализации всего датасета, которая обычно делается перед началом обучения. Дополнительно этот слой сохраняет скользящии средние mean и std, которые затем используются в режиме `eval` даже если пример в батче один.
* `norm == 2` - **LayerNormChannels**(C) - среднее по каналам (`dims=C`). Эта нормировка рассматривет индекс `C` как индекс вектора признаков (каким он и является в конечном итоге). Среднее значение этого вектора делается нулевым, а std - единичным. Затем обычаемые константы его именяют различным образом для каждой компоненты вектора.
* `norm == 3` - nn.**InstanceNorm2d**(C) - среднее по изображению (`dims=H,W`). В этом случае для каждого примера и канала усредняются яроксти всех пикселей. Нормированные изображения подправляются константтами `alpha` и `beta` имеющих уникальные значения для каждого канала (но общие для всех пикселей и примеров)

Для всех блоков используется один и тот же активационный слой, определяемый параметром `cfg.norm`.
Он же будет поставлен на место буквы `n` при описании архитектуры.
После буквы `n` можно поставить номер нормализатора. В этом случае он изменится от значения по умолчанию.
```python
"c32 n"  # после конволюции будет нормализационный слой cfg.norm
"c32 n2" # после конволюции будет *LayerNormChannels, независимо от значения cfg.norm
```

<hr>

## Активационная функция

Этот параметр является строкой, которая может принимать следующие значения:
```python
'relu', 'gelu', 'relu6', 'sigmoid', 'tanh', 'swish', 'hswish', 'hsigmoid'
```
Как и в случае нормализции функция задается единым параметром `fun` для всех блоков.
Однако, при желании, её можно указать явным образом со значением отличным от значения по умолчанию:
```python
"r n f"     # на место f будет поставлена активационная функция из cfg.fun
"r n gelu"  # независимо от cfg.fun тут будет использоваться функция GELU
```

<hr>

## Варианты CNN-архитектуры




